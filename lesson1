Naive Bayes algorithm:
For obtaining linear decision surfaces

from sklearn.naive_bayes import GaussianNB (import clasiffiying function)
clf = GaussianNB()(create clasificatory function)
clf.fit(X, Y)(fits the function to our features X and labels Y)
predict(self, X)(we call for a prediction)
clf.score(X) returns accuracy of given data and labels


### Inferencia

Bayes Rule

Calculate the probability of occurring an event given some test that depending on its outcome gives more or less certainty of that event occurring...

Example:

Probability of a certain type of cancer is 0.01 among all the population.
If a test is run a positive test gives 0.9 probability of cancer and 0.1 on no cancer
A negative test gives however 0.9 probability of not having cancer and 0.1 of having

Which is the probability of actually having cancer if the test is positive?

Prior probabilities:

P(C) = 0.01 "probability of cancer"
P(Pos / C) = 0.9 "provability of cancer with positive test"
P(Neg / C) = 0.1 "provability of cancer with negative test"
P(Pos / NC) = 0.1 "provability of positive test without cancer"
P(Neg / NC) = 0.9 "provability of negative test without cancer"

Joint probabilities:

P(C / Pos) = P(C) * P(Pos / C) = 0.009 "Probability of having cancer with a positive test"
P(NC / Pos) = P(NC) * P(Pos / NC) = 0.099 "Provability of no cancer with positiove test"

Normalizer:

Norm = P(C / Pos) + P(NC / Pos) = 0.108

Posterior probability:

P(C / Pos) = P(C / Pos)/Norm = 0.0833
P(NC / Pos) = P(NC / Pos)/Norm = 0.9167

Probability of cancer with positive result = P(C / Pos) = 0.0833
Probability of no cancer with negative result = P(NC / Pos) = 0.9167

Naive Bayes when to use:

Example email clasiffication:

The probability of an email being sent by person A or B given the probability of A or B using certain words
should be = the total probability of A or B sending the email multiplied by the probability of each word
Prob (A) = P(A) * P(A / word1)^n.times * P(A / word2)^n.times...รง
Normalizer = P(A)+P(B)
Real probability (A) = P(A) / Normalizer

This means that the algorithm does not take into account for example word order so if
word order is relevant in our problem the algorithm wont work (*)

Pros:
- Easy to implement
- Efficient
- Really accurate depending on the problem

Cons:
- It may fail depending on the problem(*)

  
